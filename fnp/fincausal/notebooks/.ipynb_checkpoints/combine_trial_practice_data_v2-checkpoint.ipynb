{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import uuid\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fincausal_task1_trial_path = Path('/media/sarthak/HDD/data_science/fnp_resources/data/task1/v2/trial.csv')\n",
    "fincausal_task1_practice_path = Path('/media/sarthak/HDD/data_science/fnp_resources/data/task1/v2/practice.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/sarthak/HDD/Anaconda/envs/fnp/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/media/sarthak/HDD/Anaconda/envs/fnp/lib/python3.6/site-packages/ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "fincausal_task1_trial = pd.read_csv(fincausal_task1_trial_path, sep='; ')\n",
    "fincausal_task1_practice = pd.read_csv(fincausal_task1_practice_path, sep='; ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Text</th>\n",
       "      <th>Gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00001</td>\n",
       "      <td>Third Democratic presidential debate  Septembe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.00002</td>\n",
       "      <td>On the policy front, Bernie Sanders claimed hi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.00003</td>\n",
       "      <td>Joe Biden misrepresented recent history when h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.00004</td>\n",
       "      <td>Here's a look at some of the assertions in the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.00005</td>\n",
       "      <td>It killed 22 people, and injured many more, we...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Index                                               Text  Gold\n",
       "0  1.00001  Third Democratic presidential debate  Septembe...     0\n",
       "1  1.00002  On the policy front, Bernie Sanders claimed hi...     0\n",
       "2  1.00003  Joe Biden misrepresented recent history when h...     0\n",
       "3  1.00004  Here's a look at some of the assertions in the...     0\n",
       "4  1.00005  It killed 22 people, and injured many more, we...     0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fincausal_task1_trial.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Text</th>\n",
       "      <th>Gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00001</td>\n",
       "      <td>Florida raking in billions as Americans abando...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.00002</td>\n",
       "      <td>Recently, changes to the U.S. tax code have en...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.00003</td>\n",
       "      <td>MORE FROM FOXBUSINESS.COM... As it turns out, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.00004</td>\n",
       "      <td>According to a new study from LendingTree, whi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.00005</td>\n",
       "      <td>The Sunshine State drew in a net influx of abo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Index                                               Text  Gold\n",
       "0  1.00001  Florida raking in billions as Americans abando...     0\n",
       "1  1.00002  Recently, changes to the U.S. tax code have en...     0\n",
       "2  1.00003  MORE FROM FOXBUSINESS.COM... As it turns out, ...     0\n",
       "3  1.00004  According to a new study from LendingTree, whi...     0\n",
       "4  1.00005  The Sunshine State drew in a net influx of abo...     1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fincausal_task1_practice.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13478, 3) (8580, 3)\n"
     ]
    }
   ],
   "source": [
    "print(fincausal_task1_practice.shape, fincausal_task1_trial.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were whitexpaces in column name when using sep=';', which is now removed. We must save them with whitespaces removed to avoid coding the logic in package code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index    0\n",
       "Text     3\n",
       "Gold     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NAs practice\n",
    "fincausal_task1_practice.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index    0\n",
       "Text     1\n",
       "Gold     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NAs trial\n",
    "fincausal_task1_trial.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some NAs in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many? 0\n",
      "label distribution for them: Series([], Name: Gold, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# empty sentences practice?\n",
    "empty_sections_practice = fincausal_task1_practice.loc[(fincausal_task1_practice.Text == \" \") | (fincausal_task1_practice.Text == \"\")]\n",
    "print('how many? {}'.format(len(empty_sections_practice)))\n",
    "print('label distribution for them: {}'.format(empty_sections_practice.Gold.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many? 0\n",
      "label distribution for them: Series([], Name: Gold, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# empty sentences trial?\n",
    "empty_sections_trial = fincausal_task1_trial.loc[(fincausal_task1_trial.Text == \" \") | (fincausal_task1_trial.Text == \"\")]\n",
    "print('how many? {}'.format(len(empty_sections_trial)))\n",
    "print('label distribution for them: {}'.format(empty_sections_trial.Gold.value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 1 empty text in trial data and 3 in practice data. All of them have 0 labels. We keep them becaue they are probable instances during the inference time. However, we remove them from training the model as they can disturb the training process, instead just return 0 when they arrive during inference time\n",
    "\n",
    "UPDATE: There are no empty text sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label distribution of trial: 0    8011\n",
      "1     569\n",
      "Name: Gold, dtype: int64\n",
      "label distribution of practice: 0    12468\n",
      "1     1010\n",
      "Name: Gold, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# label distributions in both the sets? - both have roughly same distribution\n",
    "fincausal_task1_trial_label_distribution = fincausal_task1_trial.Gold.value_counts()\n",
    "fincausal_task1_practice_label_distribution = fincausal_task1_practice.Gold.value_counts()\n",
    "\n",
    "print('label distribution of trial: {}'.format(fincausal_task1_trial_label_distribution))\n",
    "print('label distribution of practice: {}'.format(fincausal_task1_practice_label_distribution))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trial has 6.63% positives, and practice has 7.49% positives. We must use that for class weighting, as well as maintain the distribution while splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do they have any EXACTLY same texts?\n",
    "len(list(set(fincausal_task1_trial.Text).intersection(set(fincausal_task1_practice.Text))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 206 exactly same texts between practice and trial, and we keep them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index_trial</th>\n",
       "      <th>Text</th>\n",
       "      <th>Gold_trial</th>\n",
       "      <th>Index_practice</th>\n",
       "      <th>Gold_practice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.00007</td>\n",
       "      <td>The current ratio, also known as the working c...</td>\n",
       "      <td>0</td>\n",
       "      <td>316.00076</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.00008</td>\n",
       "      <td>The ratio is simply calculated by dividing cur...</td>\n",
       "      <td>0</td>\n",
       "      <td>316.00077</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.00009</td>\n",
       "      <td>Typically, the higher the current ratio the be...</td>\n",
       "      <td>0</td>\n",
       "      <td>316.00078</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.00018</td>\n",
       "      <td>Enterprise Value is calculated by taking the m...</td>\n",
       "      <td>0</td>\n",
       "      <td>316.00089</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.00019</td>\n",
       "      <td>The average FCF of a company is determined by ...</td>\n",
       "      <td>0</td>\n",
       "      <td>316.00090</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index_trial                                               Text  Gold_trial  \\\n",
       "0      2.00007  The current ratio, also known as the working c...           0   \n",
       "1      2.00008  The ratio is simply calculated by dividing cur...           0   \n",
       "2      2.00009  Typically, the higher the current ratio the be...           0   \n",
       "3      2.00018  Enterprise Value is calculated by taking the m...           0   \n",
       "4      2.00019  The average FCF of a company is determined by ...           0   \n",
       "\n",
       "   Index_practice  Gold_practice  \n",
       "0       316.00076              0  \n",
       "1       316.00077              0  \n",
       "2       316.00078              0  \n",
       "3       316.00089              0  \n",
       "4       316.00090              0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# are the labels for the equal texts same?\n",
    "merged_inner = pd.merge(left=fincausal_task1_trial, \n",
    "                        right=fincausal_task1_practice, \n",
    "                        left_on='Text', \n",
    "                        right_on='Text',\n",
    "                        suffixes=('_trial', '_practice'))\n",
    "merged_inner.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_inner.Gold_trial.equals(merged_inner.Gold_practice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index_trial</th>\n",
       "      <th>Text</th>\n",
       "      <th>Gold_trial</th>\n",
       "      <th>Index_practice</th>\n",
       "      <th>Gold_practice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Index_trial, Text, Gold_trial, Index_practice, Gold_practice]\n",
       "Index: []"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what rows are there which have different labels for same text?\n",
    "merged_inner.loc[merged_inner.Gold_trial != merged_inner.Gold_practice]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are the 0 rows which have same texts but different golds, we must remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do trial and practice dfs have common indexes?\n",
    "common_indexes = list(set(fincausal_task1_practice.Index).intersection(fincausal_task1_trial.Index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4561"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is the text also equal?\n",
    "fincausal_task1_practice.loc[fincausal_task1_practice.Index.isin(common_indexes)].Text.equals(fincausal_task1_trial.loc[fincausal_task1_trial.Index.isin(common_indexes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4561 common index values between trial and practice, and might be a problem after concatenating to have unique values. Therefore, create your own unique ID after concatenating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([fincausal_task1_practice, fincausal_task1_trial])\n",
    "all_data['unique_id'] = [str(uuid.uuid4()) for _ in range(len(all_data))]\n",
    "all_data = all_data[~all_data.Text.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index        0\n",
       "Text         0\n",
       "Gold         0\n",
       "unique_id    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22054, 4)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.928403\n",
       "1    0.071597\n",
       "Name: Gold, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.Gold.value_counts() / len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8580, 3) (13478, 3)\n",
      "(8331, 3) (13179, 3)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# 1. remove the indexes with empty text\n",
    "fincausal_task1_trial_nonemptytext = fincausal_task1_trial.loc[~fincausal_task1_trial.Index.isin(empty_sections_trial)]\n",
    "fincausal_task1_practice_nonemptytext = fincausal_task1_practice.loc[~fincausal_task1_practice.Index.isin(empty_sections_practice)]\n",
    "print(fincausal_task1_trial_nonemptytext.shape, fincausal_task1_practice_nonemptytext.shape)\n",
    "\n",
    "# 2. remove the indexes from trial and practice (with conflicting labels) before combining\n",
    "fincausal_task1_trial_nonemptytext_noconflictgold = fincausal_task1_trial_nonemptytext.loc[~fincausal_task1_trial_nonemptytext.Index.isin(merged_inner.Index_trial)]\n",
    "fincausal_task1_practice_nonemptytext_noconflictgold = fincausal_task1_practice_nonemptytext.loc[~fincausal_task1_practice_nonemptytext.Index.isin(merged_inner.Index_practice)]\n",
    "print(fincausal_task1_trial_nonemptytext_noconflictgold.shape, fincausal_task1_practice_nonemptytext_noconflictgold.shape)\n",
    "\n",
    "# 3. trial and practice data can be combined\n",
    "fincausal_task1_combined_nonemptytext_noconflictgold = pd.concat([fincausal_task1_trial_nonemptytext_noconflictgold, fincausal_task1_practice_nonemptytext_noconflictgold])\n",
    "\n",
    "# 4. Create a unique ID column\n",
    "fincausal_task1_combined_nonemptytext_noconflictgold['unique_id'] = [str(uuid.uuid4()) for _ in range(len(fincausal_task1_combined_nonemptytext_noconflictgold))]\n",
    "\n",
    "# 5. shuffle\n",
    "fincausal_task1_combined_nonemptytext_noconflictgold = fincausal_task1_combined_nonemptytext_noconflictgold.sample(frac=1).reset_index(drop=True)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17863, 4) (1985, 4) (2206, 4)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(all_data, stratify=all_data.Gold, random_state=42, test_size=0.1)\n",
    "train, dev = train_test_split(train, stratify=train.Gold, random_state=42, test_size=0.1)\n",
    "print(train.shape, dev.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.928399\n",
       "1    0.071601\n",
       "Name: Gold, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.Gold.value_counts() / len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.928463\n",
       "1    0.071537\n",
       "Name: Gold, dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev.Gold.value_counts() / len(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.928377\n",
       "1    0.071623\n",
       "Name: Gold, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.Gold.value_counts() / len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('/media/sarthak/HDD/data_science/fnp_resources/data/task1/all_combined/train.csv', index=False)\n",
    "dev.to_csv('/media/sarthak/HDD/data_science/fnp_resources/data/task1/all_combined/dev.csv', index=False)\n",
    "test.to_csv('/media/sarthak/HDD/data_science/fnp_resources/data/task1/all_combined/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-fold splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. divide the data into k-folds with each fold having representative distribution\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dfs = []\n",
    "test_dfs = []\n",
    "\n",
    "X = fincausal_task1_combined_nonemptytext_noconflictgold\n",
    "y = fincausal_task1_combined_nonemptytext_noconflictgold.Gold\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    train = fincausal_task1_combined_nonemptytext_noconflictgold.iloc[train_index]\n",
    "    test = fincausal_task1_combined_nonemptytext_noconflictgold.iloc[test_index]\n",
    "    \n",
    "    train_dfs.append(train)\n",
    "    test_dfs.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. for each train_df, create a val_df, and save train, val and test dfs as csv\n",
    "data_root_dir = Path('/media/sarthak/HDD/data_science/fnp_resources/data/task1')\n",
    "for i, (train_df, test_df) in enumerate(zip(train_dfs, test_dfs)):\n",
    "    train_df, val_df = train_test_split(train_df, \n",
    "                                        test_size=0.1, \n",
    "                                        stratify=train_df.Gold,\n",
    "                                        random_state=42)\n",
    "    iteration_dir = data_root_dir / str('iteration_{}').format(i+1)\n",
    "    if not os.path.exists(iteration_dir):\n",
    "        os.mkdir(iteration_dir)\n",
    "        train_df.to_csv(Path(iteration_dir / 'train.csv'), index=False)\n",
    "        val_df.to_csv(Path(iteration_dir / 'val.csv'), index=False)\n",
    "        test_df.to_csv(Path(iteration_dir / 'test.csv'), index=False)\n",
    "    else:\n",
    "        raise Exception('Iteration dir already exists. Delete the directory first.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python fnp",
   "language": "python",
   "name": "fnp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
